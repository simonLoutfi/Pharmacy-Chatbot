
import time
import os
from dataclasses import dataclass
from typing import Optional, List, Tuple

import ollama
from ollama import Client

from config import config
from exceptions import CodeGenerationError, ErrorCode
from logger import llm_logger
from utils import (
    extract_code_from_response,
    build_code_generation_prompt_with_history,
    build_response_prompt_with_history
)


@dataclass
class GenerationResult:
    """Result of code generation."""
    success: bool
    code: str
    raw_response: str
    generation_time_ms: float
    model: str
    error: Optional[str] = None


class CodeGenerator:
    """
    LLM-based code generator using Ollama/DeepSeek Coder.
    
    Features:
    - 100% offline operation via Ollama
    - DeepSeek Coder 6.7B for pandas code generation
    - Automatic code extraction from responses
    - Retry logic for failed generations
    """
    
    def __init__(
        self,
        model: str = None,
        base_url: str = None,
        temperature: float = None,
        timeout: int = None
    ):
        """
        Initialize the code generator.
        
        Args:
            model: Ollama model name (default: deepseek-coder:6.7b)
            base_url: Ollama server URL (default: localhost:11434)
            temperature: Generation temperature (default: 0.1)
            timeout: Request timeout in seconds
        """
        env_model = os.environ.get("OLLAMA_MODEL")
        env_host = os.environ.get("OLLAMA_HOST")
        self.model = model or env_model or config.llm.model_name
        self.base_url = base_url or env_host or config.llm.base_url
        self.temperature = temperature if temperature is not None else config.llm.temperature
        self.timeout = timeout or config.llm.timeout
        
        # Initialize Ollama client
        self.client = _get_ollama_client(self.base_url)
        
        llm_logger.info(
            "Code generator initialized",
            model=self.model,
            base_url=self.base_url
        )
    
    def generate(self, prompt: str, max_retries: int = 2) -> GenerationResult:
        """
        Generate pandas code from a prompt.
        
        Args:
            prompt: Complete prompt with schema and question
            max_retries: Number of retries on failure
            
        Returns:
            GenerationResult with generated code
        """
        start_time = time.perf_counter()
        last_error = None
        
        model_name, available = _resolve_model_name(self.client, self.model)

        for attempt in range(max_retries + 1):
            try:
                llm_logger.info(
                    "Generating code",
                    attempt=attempt + 1,
                    prompt_length=len(prompt)
                )
                
                # Call Ollama API
                response = self.client.generate(
                    model=model_name,
                    prompt=prompt,
                    options={
                        'temperature': self.temperature,
                        'num_predict': config.llm.max_tokens,
                    }
                )
                
                raw_response = response.get('response', '')
                
                if not raw_response:
                    raise CodeGenerationError(
                        message="Empty response from LLM",
                        code=ErrorCode.LLM_INVALID_RESPONSE
                    )
                
                # Extract code from response
                code = extract_code_from_response(raw_response)
                
                if not code or len(code.strip()) < 5:
                    raise CodeGenerationError(
                        message="Could not extract valid code from response",
                        code=ErrorCode.CODE_EXTRACTION_FAILED,
                        details=raw_response[:200]
                    )
                
                generation_time = (time.perf_counter() - start_time) * 1000
                
                llm_logger.performance(
                    "code_generation",
                    generation_time,
                    code_length=len(code),
                    response_length=len(raw_response)
                )
                
                return GenerationResult(
                    success=True,
                    code=code,
                    raw_response=raw_response,
                    generation_time_ms=generation_time,
                    model=model_name
                )
                
            except ollama.ResponseError as e:
                message = str(e)
                lower = message.lower()
                if "unauthorized" in lower or "status code: 401" in lower or "401" in lower:
                    raise CodeGenerationError(
                        message=(
                            "Ollama request unauthorized. Set OLLAMA_API_KEY for cloud models "
                            "and OLLAMA_HOST if you are not using the local server."
                        ),
                        code=ErrorCode.LLM_CONNECTION_ERROR,
                        details=message
                    )
                if "not found" in lower and "model" in lower:
                    hint = ""
                    if available:
                        preview = ", ".join(sorted(available)[:6])
                        hint = f" Available models: {preview}."
                    raise CodeGenerationError(
                        message=(
                            f"Model '{self.model}' not available locally.{hint} "
                            f"Run `ollama pull {self.model}`."
                        ),
                        code=ErrorCode.LLM_INVALID_RESPONSE,
                        details=message
                    )

                last_error = f"Ollama error: {message}"
                llm_logger.warning(
                    "Ollama response error",
                    attempt=attempt + 1,
                    error=message
                )
                
            except ConnectionError as e:
                last_error = f"Connection error: {str(e)}"
                llm_logger.error(
                    "Cannot connect to Ollama",
                    error=str(e)
                )
                raise CodeGenerationError(
                    message="Cannot connect to Ollama. Is it running?",
                    code=ErrorCode.LLM_CONNECTION_ERROR,
                    details=str(e)
                )
                
            except CodeGenerationError:
                raise
                
            except Exception as e:
                message = str(e)
                lower = message.lower()
                if (
                    "connect" in lower
                    or "connection" in lower
                    or "refused" in lower
                    or "failed to connect" in lower
                ):
                    raise CodeGenerationError(
                        message="Ollama server not reachable. Ensure Ollama is running.",
                        code=ErrorCode.LLM_CONNECTION_ERROR,
                        details=message
                    )
                last_error = message
                llm_logger.warning(
                    "Generation attempt failed",
                    attempt=attempt + 1,
                    error=message
                )
        
        # All retries failed
        generation_time = (time.perf_counter() - start_time) * 1000
        
        llm_logger.error(
            "Code generation failed after retries",
            retries=max_retries,
            last_error=last_error
        )
        
        return GenerationResult(
            success=False,
            code="",
            raw_response="",
            generation_time_ms=generation_time,
            model=model_name,
            error=last_error
        )
    
    def generate_from_question(
        self,
        question: str,
        schema: str,
        history: str = ""
    ) -> GenerationResult:
        """
        Generate code from a user question and DataFrame schema.
        
        Args:
            question: User's natural language question
            schema: DataFrame schema description
            
        Returns:
            GenerationResult with generated code
        """
        prompt = build_code_generation_prompt_with_history(question, schema, history)
        return self.generate(prompt)
    
    def check_connection(self) -> bool:
        """
        Check if Ollama is running and model is available.
        
        Returns:
            True if connection successful, False otherwise
        """
        try:
            available_models = _list_available_models(self.client)
            model_name = _resolve_model_name_from_list(self.model, available_models)

            if not available_models:
                llm_logger.info(
                    "Ollama connection successful (model list empty)",
                    model=self.model
                )
                return True

            if model_name not in available_models:
                try:
                    model_info = self.client.show(self.model)
                    if model_info:
                        llm_logger.info(
                            "Model found via show() method",
                            model=self.model
                        )
                        return True
                except Exception:
                    pass

                llm_logger.warning(
                    "Model not found",
                    model=self.model,
                    available=available_models[:5]
                )
                return False

            llm_logger.info(
                "Ollama connection successful",
                model=self.model,
                available_models=len(available_models)
            )
            return True

        except Exception as e:
            llm_logger.error(
                "Ollama connection failed",
                error=str(e),
                error_type=type(e).__name__
            )
            return False
    
    def get_model_info(self) -> Optional[dict]:
        """
        Get information about the current model.
        
        Returns:
            Model information dict or None
        """
        try:
            return self.client.show(self.model)
        except Exception:
            return None


class ResponseGenerator:
    """
    Generates natural language responses from execution results.
    
    Uses the same LLM to format results into human-readable responses.
    """
    
    def __init__(self, code_generator: CodeGenerator = None):
        """Initialize with existing code generator or create new one."""
        self.generator = code_generator or CodeGenerator()
    
    def generate_response(
        self, 
        question: str, 
        results: str, 
        code: str,
        history: str = ""
    ) -> str:
        """
        Generate a natural language response explaining the results.
        
        Args:
            question: Original user question
            results: Execution results as string
            code: The executed code
            
        Returns:
            Natural language explanation
        """
        prompt = build_response_prompt_with_history(question, results, code, history)
        
        try:
            result = self.generator.generate(prompt)
            
            if result.success:
                # Clean up the response
                response = result.raw_response.strip()
                # Remove any code blocks that might have been included
                if '```' in response:
                    # Keep only text before first code block
                    parts = response.split('```')
                    response = parts[0].strip()
                    if len(parts) > 2:
                        # Add text after last code block
                        response += '\n' + parts[-1].strip()
                
                return response if response else self._fallback_response(results)
            else:
                return self._fallback_response(results)
                
        except Exception as e:
            llm_logger.warning(
                "Response generation failed, using fallback",
                error=str(e)
            )
            return self._fallback_response(results)
    
    def _fallback_response(self, results: str) -> str:
        """Generate a basic fallback response."""
        return f"Here are the results of your analysis:\n\n{results}"


# Global instances
_generator: Optional[CodeGenerator] = None
_response_generator: Optional[ResponseGenerator] = None


def get_code_generator() -> CodeGenerator:
    """Get or create the global code generator."""
    global _generator
    if _generator is None:
        _generator = CodeGenerator()
    return _generator


def get_response_generator() -> ResponseGenerator:
    """Get or create the global response generator."""
    global _response_generator
    if _response_generator is None:
        _response_generator = ResponseGenerator(get_code_generator())
    return _response_generator


def generate_code(question: str, schema: str) -> GenerationResult:
    """
    Convenience function for code generation.
    
    Args:
        question: User's question
        schema: DataFrame schema
        
    Returns:
        GenerationResult with generated code
    """
    return get_code_generator().generate_from_question(question, schema)


def _get_ollama_client(base_url: str) -> Client:
    host = os.environ.get("OLLAMA_HOST") or base_url
    api_key = os.environ.get("OLLAMA_API_KEY")
    headers = {"Authorization": f"Bearer {api_key}"} if api_key else None
    if headers:
        return Client(host=host, headers=headers)
    return Client(host=host)


def _list_available_models(ollama_client) -> List[str]:
    try:
        payload = ollama_client.list()
    except Exception:
        return []

    models = payload.get("models", []) if isinstance(payload, dict) else payload
    if not isinstance(models, list):
        return []

    names = []
    for model in models:
        if isinstance(model, dict):
            name = (
                model.get("name")
                or model.get("model")
                or model.get("model_name")
            )
        elif isinstance(model, str):
            name = model
        else:
            name = None
        if name:
            names.append(name)
    return names


def _resolve_model_name(ollama_client, requested: str) -> Tuple[str, List[str]]:
    available = _list_available_models(ollama_client)
    if not available:
        return requested, available
    return _resolve_model_name_from_list(requested, available), available


def _resolve_model_name_from_list(requested: str, available: List[str]) -> str:
    if requested in available:
        return requested

    requested_base = requested.split(":", 1)[0]
    for name in available:
        if name.split(":", 1)[0] == requested_base:
            return name

    return requested

